#!/bin/bash
set -euo pipefail

# =============================================================================
# Buildkite Pre-Command Hook - Slurm Job Submission
# =============================================================================
# This hook intercepts Buildkite commands and submits them to Slurm instead
# of running locally. Customize partition names and constraints below.
# =============================================================================

# Allow override via pipeline env vars
PARTITION="${BUILDKITE_SLURM_PARTITION:-}"
GRES="${BUILDKITE_SLURM_GRES:-}"
TIME_LIMIT="${BUILDKITE_SLURM_TIME:-01:00:00}"
NODES="${BUILDKITE_SLURM_NODES:-1}"
NTASKS_PER_NODE="${BUILDKITE_SLURM_NTASKS_PER_NODE:-1}"
CPUS_PER_TASK="${BUILDKITE_SLURM_CPUS_PER_TASK:-1}"

# =============================================================================
# Build Slurm submission script
# =============================================================================
SBATCH_SCRIPT=$(mktemp)
LOG_FILE="${BUILDKITE_BUILD_CHECKOUT_PATH}/slurm-\${SLURM_JOB_ID}.log"

# optional partition line
PARTITION_LINE=""
if [ -n "$PARTITION" ]; then
  PARTITION_LINE="#SBATCH --partition=${PARTITION}"
fi

# Build optional SBATCH directives
GRES_LINE=""
if [ -n "$GRES" ]; then
  GRES_LINE="#SBATCH --gres=${GRES}"
fi

cat >"$SBATCH_SCRIPT" <<SBATCH_EOF
#!/bin/bash
#SBATCH --job-name=buildkite-${BUILDKITE_BUILD_NUMBER}-${BUILDKITE_STEP_KEY}
${PARTITION_LINE}
#SBATCH --nodes=${NODES}
#SBATCH --ntasks-per-node=${NTASKS_PER_NODE}
#SBATCH --cpus-per-task=${CPUS_PER_TASK}
${GRES_LINE}
#SBATCH --time=${TIME_LIMIT}
#SBATCH --output=${LOG_FILE}
#SBATCH --error=${LOG_FILE}

# Load any required modules
# Uncomment and customize as needed:
# module load enroot pyxis
# module load cuda/12.0
# module load rocm/6.0

# Change to build directory
cd ${BUILDKITE_BUILD_CHECKOUT_PATH}

# Execute the buildkite command
set -euo pipefail
${BUILDKITE_COMMAND}
SBATCH_EOF

# =============================================================================
# Submit to Slurm and monitor
# =============================================================================
echo "--- :slurm: Submitting to Slurm"
echo "Partition: ${PARTITION}"
if [ -n "$GRES" ]; then
  echo "GRES: ${GRES}"
fi
echo "Time Limit: ${TIME_LIMIT}"

JOB_ID=$(sbatch --parsable "$SBATCH_SCRIPT")
echo "Job ID: $JOB_ID"
echo

# Wait for job to start
echo "Waiting for job to start..."
while true; do
  STATE=$(squeue -j $JOB_ID -h -o %T 2>/dev/null || echo "UNKNOWN")
  if [ "$STATE" = "RUNNING" ]; then
    echo "Job is running"
    break
  elif [ "$STATE" = "PENDING" ]; then
    sleep 2
  elif [ "$STATE" = "UNKNOWN" ]; then
    # Job might have already finished
    break
  else
    echo "Job state: $STATE"
    sleep 2
  fi
done

# Tail logs in real-time
echo
echo "--- :scroll: Job Output"
# Wait a moment for log file to be created
sleep 2

# Tail logs (will exit when job completes)
tail -f "${LOG_FILE}" 2>/dev/null &
TAIL_PID=$!

# Wait for job completion
while squeue -j $JOB_ID -h >/dev/null 2>&1; do
  sleep 5
done

# Give tail a moment to catch up
sleep 2
kill $TAIL_PID 2>/dev/null || true
wait $TAIL_PID 2>/dev/null || true

# Get final job status
echo
echo "--- :mag: Job Status"
sacct -j $JOB_ID --format=JobID,State,ExitCode,Elapsed,MaxRSS,AllocTRES -P

# Check exit code
SLURM_EXIT=$(sacct -j $JOB_ID --format=ExitCode --noheader | head -1 | awk -F: '{print $1}' | tr -d ' ')

# Cleanup
rm -f "$SBATCH_SCRIPT"

if [ "$SLURM_EXIT" != "0" ]; then
  echo
  echo "Job failed with exit code: $SLURM_EXIT"
  exit $SLURM_EXIT
fi

echo
echo "Job completed successfully"
